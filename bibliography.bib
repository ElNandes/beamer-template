@Article{art1,
	author =       "Author Name",
	title =        "Book Title",
	journal =      "Lecture Notes in Autonomous System",
	volume =       "1001",
	publisher =    "UFO",
	pages =        "900--921",
	year =         "2003",
	coden =        "LNCSD9",
	ISSN =         "0302-2345",
	bibdate =      "Sat Dec 7 10:05:42 MST 2003",
}


@inproceedings{horbach-palmer-2016-investigating,
	title = "Investigating Active Learning for Short-Answer Scoring",
	author = "Horbach, Andrea  and
	Palmer, Alexis",
	booktitle = "Proceedings of the 11th Workshop on Innovative Use of {NLP} for Building Educational Applications",
	month = jun,
	year = "2016",
	address = "San Diego, CA",
	publisher = "Association for Computational Linguistics",
	url = "https://aclanthology.org/W16-0535",
	doi = "10.18653/v1/W16-0535",
	pages = "301--311",
}

@inproceedings{mieskes-pado-2018-work,
	title = "Work Smart - Reducing Effort in Short-Answer Grading",
	author = "Mieskes, Margot  and
	Pad{\'o}, Ulrike",
	booktitle = "Proceedings of the 7th workshop on {NLP} for Computer Assisted Language Learning",
	month = nov,
	year = "2018",
	address = "Stockholm, Sweden",
	publisher = "LiU Electronic Press",
	url = "https://aclanthology.org/W18-7107",
	pages = "57--68",
}


@inproceedings{dzikovska-etal-2013-semeval,
	title = "{S}em{E}val-2013 Task 7: The Joint Student Response Analysis and 8th Recognizing Textual Entailment Challenge",
	author = "Dzikovska, Myroslava  and
	Nielsen, Rodney  and
	Brew, Chris  and
	Leacock, Claudia  and
	Giampiccolo, Danilo  and
	Bentivogli, Luisa  and
	Clark, Peter  and
	Dagan, Ido  and
	Dang, Hoa Trang",
	booktitle = "Second Joint Conference on Lexical and Computational Semantics (*{SEM}), Volume 2: Proceedings of the Seventh International Workshop on Semantic Evaluation ({S}em{E}val 2013)",
	month = jun,
	year = "2013",
	address = "Atlanta, Georgia, USA",
	publisher = "Association for Computational Linguistics",
	url = "https://aclanthology.org/S13-2045",
	pages = "263--274",
}

@inproceedings{mohler-etal-2011-learning,
	title = "Learning to Grade Short Answer Questions using Semantic Similarity Measures and Dependency Graph Alignments",
	author = "Mohler, Michael  and
	Bunescu, Razvan  and
	Mihalcea, Rada",
	booktitle = "Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies",
	month = jun,
	year = "2011",
	address = "Portland, Oregon, USA",
	publisher = "Association for Computational Linguistics",
	url = "https://aclanthology.org/P11-1076",
	pages = "752--762",
}

@article{Meurers-2011a,
	abstract = { Contextualised, meaning-based interaction in the foreign language is widely recognised as crucial for second language acquisition. Correspondingly, current exercises in foreign language teaching generally require students to manipulate both form and meaning. For intelligent language tutoring systems to support such activities, they thus must be able to evaluate the appropriateness of the meaning of a learner response for a given exercise. We discuss such a content-assessment approach, focusing on reading comprehension exercises. We pursue the idea that a range of simultaneously available representations at different levels of complexity and linguistic abstraction provide a good empirical basis for content assessment. We show how an annotation-based NLP architecture implementing this idea can be realised and that it successfully performs on a corpus of authentic learner answers to reading comprehension questions. To support comparison and sustainable development on content assessment, we also define a general exchange format for such exercise data. },
	author = {Meurers, Detmar and Ziai, Ramon and Ott, Niels and Bailey, Stacey M.},
	doi = {10.1504/IJCEELL.2011.042793},
	eprint = {https://www.inderscienceonline.com/doi/pdf/10.1504/IJCEELL.2011.042793},
	journal = {International Journal of Continuing Engineering Education and Life Long Learning},
	note = {PMID: 42793},
	number = {4},
	pages = {355-369},
	title = {Integrating parallel analysis modules to evaluate the meaning of answers to reading comprehension questions},
	url = {https://www.inderscienceonline.com/doi/abs/10.1504/IJCEELL.2011.042793},
	volume = {21},
	year = {2011},
	bdsk-url-1 = {https://www.inderscienceonline.com/doi/abs/10.1504/IJCEELL.2011.042793},
	bdsk-url-2 = {https://doi.org/10.1504/IJCEELL.2011.042793}
}

@inproceedings{meurers-etal-2011-evaluating,
	title = "Evaluating Answers to Reading Comprehension Questions in Context: Results for {G}erman and the Role of Information Structure",
	author = "Meurers, Detmar  and
	Ziai, Ramon  and
	Ott, Niels  and
	Kopp, Janina",
	booktitle = "Proceedings of the {T}ext{I}nfer 2011 Workshop on Textual Entailment",
	month = jul,
	year = "2011",
	address = "Edinburgh, Scottland, UK",
	publisher = "Association for Computational Linguistics",
	url = "https://aclanthology.org/W11-2401",
	pages = "1--9",
}


@inproceedings{pado-kiefer-2015-short,
	title = "Short Answer Grading: When Sorting Helps and When it Doesn{'}t",
	author = "Pado, Ulrike  and
	Kiefer, Cornelia",
	booktitle = "Proceedings of the fourth workshop on {NLP} for computer-assisted language learning",
	month = may,
	year = "2015",
	address = "Vilnius, Lithuania",
	publisher = "LiU Electronic Press",
	url = "https://aclanthology.org/W15-1905",
	pages = "42--50",
}


@misc{Wu-et-al-2021,
	doi = {10.48550/ARXIV.2107.14035},
	
	url = {https://arxiv.org/abs/2107.14035},
	
	author = {Wu, Mike and Goodman, Noah and Piech, Chris and Finn, Chelsea},
	
	keywords = {Computers and Society (cs.CY), Machine Learning (cs.LG), FOS: Computer and information sciences, FOS: Computer and information sciences},
	
	title = {ProtoTransformer: A Meta-Learning Approach to Providing Student Feedback},
	
	publisher = {arXiv},
	
	year = {2021},
	
	copyright = {Creative Commons Attribution 4.0 International}
}

@misc{SBERT,
	doi = {10.48550/ARXIV.1908.10084},
	
	url = {https://arxiv.org/abs/1908.10084},
	
	author = {Reimers, Nils and Gurevych, Iryna},
	
	keywords = {Computation and Language (cs.CL), FOS: Computer and information sciences, FOS: Computer and information sciences},
	
	title = {Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks},
	
	publisher = {arXiv},
	
	year = {2019},
	
	copyright = {Creative Commons Attribution Share Alike 4.0 International}
}


@inproceedings{ein-dor-etal-2020-active,
	title = "{A}ctive {L}earning for {BERT}: {A}n {E}mpirical {S}tudy",
	author = "Ein-Dor, Liat  and
	Halfon, Alon  and
	Gera, Ariel  and
	Shnarch, Eyal  and
	Dankin, Lena  and
	Choshen, Leshem  and
	Danilevsky, Marina  and
	Aharonov, Ranit  and
	Katz, Yoav  and
	Slonim, Noam",
	booktitle = "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)",
	month = nov,
	year = "2020",
	address = "Online",
	publisher = "Association for Computational Linguistics",
	url = "https://aclanthology.org/2020.emnlp-main.638",
	doi = "10.18653/v1/2020.emnlp-main.638",
	pages = "7949--7962",
	abstract = "Real world scenarios present a challenge for text classification, since labels are usually expensive and the data is often characterized by class imbalance. Active Learning (AL) is a ubiquitous paradigm to cope with data scarcity. Recently, pre-trained NLP models, and BERT in particular, are receiving massive attention due to their outstanding performance in various NLP tasks. However, the use of AL with deep pre-trained models has so far received little consideration. Here, we present a large-scale empirical study on active learning techniques for BERT-based classification, addressing a diverse set of AL strategies and datasets. We focus on practical scenarios of binary text classification, where the annotation budget is very small, and the data is often skewed. Our results demonstrate that AL can boost BERT performance, especially in the most realistic scenario in which the initial set of labeled examples is created using keyword-based queries, resulting in a biased sample of the minority class. We release our research framework, aiming to facilitate future research along the lines explored here.",
}


@inproceedings{Lewis-et-al-1994,
	abstract = {The ability to cheaply train text classifiers is critical to their use in information retrieval, content analysis, natural language processing, and other tasks involving data which is partly or fully textual. An algorithm for sequential sampling during machine learning of statistical classifiers was developed and tested on a newswire text categorization task. This method, which we call uncertainty sampling, reduced by as much as 500-fold the amount of training data that would have to be manually classified to achieve a given level of effectiveness.},
	address = {London},
	author = {Lewis, David D. and Gale, William A.},
	booktitle = {SIGIR '94},
	editor = {Croft, Bruce W. and van Rijsbergen, C. J.},
	isbn = {978-1-4471-2099-5},
	pages = {3--12},
	publisher = {Springer London},
	title = {A Sequential Algorithm for Training Text Classifiers},
	year = {1994}
	
}


@InProceedings{Gal-et-al-2016,
	title = 	 {Dropout as a Bayesian Approximation: Representing Model Uncertainty in Deep Learning},
	author = 	 {Gal, Yarin and Ghahramani, Zoubin},
	booktitle = 	 {Proceedings of The 33rd International Conference on Machine Learning},
	pages = 	 {1050--1059},
	year = 	 {2016},
	editor = 	 {Balcan, Maria Florina and Weinberger, Kilian Q.},
	volume = 	 {48},
	series = 	 {Proceedings of Machine Learning Research},
	address = 	 {New York, New York, USA},
	month = 	 {20--22 Jun},
	publisher =    {PMLR},
	pdf = 	 {http://proceedings.mlr.press/v48/gal16.pdf},
	url = 	 {https://proceedings.mlr.press/v48/gal16.html},
	abstract = 	 {Deep learning tools have gained tremendous attention in applied machine learning. However such tools for regression and classification do not capture model uncertainty. In comparison, Bayesian models offer a mathematically grounded framework to reason about model uncertainty, but usually come with a prohibitive computational cost. In this paper we develop a new theoretical framework casting dropout training in deep neural networks (NNs) as approximate Bayesian inference in deep Gaussian processes. A direct result of this theory gives us tools to model uncertainty with dropout NNs – extracting information from existing models that has been thrown away so far. This mitigates the problem of representing uncertainty in deep learning without sacrificing either computational complexity or test accuracy. We perform an extensive study of the properties of dropout’s uncertainty. Various network architectures and non-linearities are assessed on tasks of regression and classification, using MNIST as an example. We show a considerable improvement in predictive log-likelihood and RMSE compared to existing state-of-the-art methods, and finish by using dropout’s uncertainty in deep reinforcement learning.}
}


@misc{Huang-et-al-2016,
	doi = {10.48550/ARXIV.1612.03226},
	
	url = {https://arxiv.org/abs/1612.03226},
	
	author = {Huang, Jiaji and Child, Rewon and Rao, Vinay and Liu, Hairong and Satheesh, Sanjeev and Coates, Adam},
	
	keywords = {Computation and Language (cs.CL), Machine Learning (cs.LG), Machine Learning (stat.ML), FOS: Computer and information sciences, FOS: Computer and information sciences},
	
	title = {Active Learning for Speech Recognition: the Power of Gradients},
	
	publisher = {arXiv},
	
	year = {2016},
	
	copyright = {arXiv.org perpetual, non-exclusive license}
}

@inproceedings{
	sener2018active,
	title={Active Learning for Convolutional Neural Networks: A Core-Set Approach},
	author={Ozan Sener and Silvio Savarese},
	booktitle={International Conference on Learning Representations},
	year={2018},
	url={https://openreview.net/forum?id=H1aIuk-RW},
}

@misc{Gissin-et-al-2019,
	doi = {10.48550/ARXIV.1907.06347},
	
	url = {https://arxiv.org/abs/1907.06347},
	
	author = {Gissin, Daniel and Shalev-Shwartz, Shai},
	
	keywords = {Machine Learning (cs.LG), Machine Learning (stat.ML), FOS: Computer and information sciences, FOS: Computer and information sciences},
	
	title = {Discriminative Active Learning},
	
	publisher = {arXiv},
	
	year = {2019},
	
	copyright = {arXiv.org perpetual, non-exclusive license}
}

@inproceedings{Heilman2013ETSDA,
	title={ETS: Domain Adaptation and Stacking for Short Answer Scoring},
	author={Michael Heilman and Nitin Madnani},
	booktitle={*SEMEVAL},
	year={2013}
}

@article{wolpert,
	author = {Wolpert, David},
	year = {1992},
	month = {12},
	pages = {241-259},
	title = {Stacked Generalization},
	volume = {5},
	journal = {Neural Networks},
	doi = {10.1016/S0893-6080(05)80023-1}
}

@inproceedings{daume-iii-2007-frustratingly,
	title = "Frustratingly Easy Domain Adaptation",
	author = "Daum{\'e} III, Hal",
	booktitle = "Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics",
	month = jun,
	year = "2007",
	address = "Prague, Czech Republic",
	publisher = "Association for Computational Linguistics",
	url = "https://aclanthology.org/P07-1033",
	pages = "256--263",
}

@inproceedings{dzikovska-etal-2013-semeval,
	title = "{S}em{E}val-2013 Task 7: The Joint Student Response Analysis and 8th Recognizing Textual Entailment Challenge",
	author = "Dzikovska, Myroslava  and
	Nielsen, Rodney  and
	Brew, Chris  and
	Leacock, Claudia  and
	Giampiccolo, Danilo  and
	Bentivogli, Luisa  and
	Clark, Peter  and
	Dagan, Ido  and
	Dang, Hoa Trang",
	booktitle = "Second Joint Conference on Lexical and Computational Semantics (*{SEM}), Volume 2: Proceedings of the Seventh International Workshop on Semantic Evaluation ({S}em{E}val 2013)",
	month = jun,
	year = "2013",
	address = "Atlanta, Georgia, USA",
	publisher = "Association for Computational Linguistics",
	url = "https://aclanthology.org/S13-2045",
	pages = "263--274",
}

@inproceedings{Ott2013CoMeTID,
	title={CoMeT: Integrating different levels of linguistic modeling for meaning assessment},
	author={Niels Ott and Ramon Ziai and Michael Hahn and Walt Detmar Meurers},
	booktitle={*SEMEVAL},
	year={2013}
}



@inproceedings{darus2001students,
	title={Students’ expectations of a computer-based essay marking system},
	author={Darus, Saadiyah and Hussin, Supyan and Stapa, Siti Hamin},
	booktitle={Reflections, visions \& dreams of practice: Selected papers from the IEC 2002 International Education Conference},
	pages={197--204},
	year={2001},
	organization={ICT Learning Sdn Bhd Kuala Lumpur}
}



@inproceedings{sultan-etal-2016-fast,
	title = "Fast and Easy Short Answer Grading with High Accuracy",
	author = "Sultan, Md Arafat  and
	Salazar, Cristobal  and
	Sumner, Tamara",
	booktitle = "Proceedings of the 2016 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies",
	month = jun,
	year = "2016",
	address = "San Diego, California",
	publisher = "Association for Computational Linguistics",
	url = "https://aclanthology.org/N16-1123",
	doi = "10.18653/v1/N16-1123",
	pages = "1070--1075",
}

@misc{BERT,
	doi = {10.48550/ARXIV.1810.04805},
	
	url = {https://arxiv.org/abs/1810.04805},
	
	author = {Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina},
	
	keywords = {Computation and Language (cs.CL), FOS: Computer and information sciences, FOS: Computer and information sciences},
	
	title = {BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding},
	
	publisher = {arXiv},
	
	year = {2018},
	
	copyright = {arXiv.org perpetual, non-exclusive license}
}


@misc{XLNET,
	doi = {10.48550/ARXIV.1906.08237},
	
	url = {https://arxiv.org/abs/1906.08237},
	
	author = {Yang, Zhilin and Dai, Zihang and Yang, Yiming and Carbonell, Jaime and Salakhutdinov, Ruslan and Le, Quoc V.},
	
	keywords = {Computation and Language (cs.CL), Machine Learning (cs.LG), FOS: Computer and information sciences, FOS: Computer and information sciences},
	
	title = {XLNet: Generalized Autoregressive Pretraining for Language Understanding},
	
	publisher = {arXiv},
	
	year = {2019},
	
	copyright = {Creative Commons Attribution 4.0 International}
}

@conference{Ghavidel,
	author={Hadi Ghavidel. and Amal Zouaq. and Michel Desmarais.},
	title={Using BERT and XLNET for the Automatic Short Answer Grading Task},
	booktitle={Proceedings of the 12th International Conference on Computer Supported Education - Volume 1: CSEDU,},
	year={2020},
	pages={58-67},
	publisher={SciTePress},
	organization={INSTICC},
	doi={10.5220/0009422400580067},
	isbn={978-989-758-417-6},
	issn={2184-5026},
}

@article{Settles2009,
	author = {Settles, Burr},
	file = {:Users/elantonfernandes/Library/Application Support/Mendeley Desktop/Downloaded/Settles - 2009 - Computer Sciences Department Active Learning Literature Survey.pdf:pdf},
	keywords = {active learning,literature survey,machine learning},
	title = {{Computer Sciences Department Active Learning Literature Survey}},
	year = {2009}
}

@article{modAL2018,
	title={mod{AL}: {A} modular active learning framework for {P}ython},
	author={Tivadar Danka and Peter Horvath},
	url={https://github.com/modAL-python/modAL},
	note={available on arXiv at \url{https://arxiv.org/abs/1805.00979}}
}

@inproceedings{akbik2018coling,
	title={Contextual String Embeddings for Sequence Labeling},
	author={Akbik, Alan and Blythe, Duncan and Vollgraf, Roland},
	booktitle = {{COLING} 2018, 27th International Conference on Computational Linguistics},
	pages     = {1638--1649},
	year      = {2018}
}

@inproceedings{akbik2019flair,
	title={FLAIR: An easy-to-use framework for state-of-the-art NLP},
	author={Akbik, Alan and Bergmann, Tanja and Blythe, Duncan and Rasul, Kashif and Schweter, Stefan and Vollgraf, Roland},
	booktitle={{NAACL} 2019, 2019 Annual Conference of the North American Chapter of the Association for Computational Linguistics (Demonstrations)},
	pages={54--59},
	year={2019}
}

@misc{schweter2020flert,
	title={FLERT: Document-Level Features for Named Entity Recognition},
	author={Stefan Schweter and Alan Akbik},
	year={2020},
	eprint={2011.06993},
	archivePrefix={arXiv},
	primaryClass={cs.CL}}

@inproceedings{halder2020coling,
	title={Task Aware Representation of Sentences for Generic Text Classification},
	author={Halder, Kishaloy and Akbik, Alan and Krapac, Josip and Vollgraf, Roland},
	booktitle = {{COLING} 2020, 28th International Conference on Computational Linguistics},
	year      = {2020}
}

@book{NLTK,
	title={Natural language processing with Python: analyzing text with the natural language toolkit},
	author={Bird, Steven and Klein, Ewan and Loper, Edward},
	year={2009},
	publisher={" O'Reilly Media, Inc."}
}

@mastersthesis{2021Gaddipati,
	author = {Gaddipati, Sasi Kiran and Pl{\"o}ger, Paul G. and Hochgeschwender, Nico and Metzler, Tim},
	title = {Automatic Formative Assessment for Students’ Short Text Answers through Feature Extraction},
	school = {Hochschule Bonn-Rhein-Sieg},
	address = {Grantham-Allee 20, 53757 St. Augustin, Germany},
	month = {April},
	year = {2021},
	note = {WS18/19 H-BRS 
	Pl{\"o}ger, Hochgeschwender, Metzler supervising}
}

@inproceedings{huguet-cabot-navigli-2021-rebel-relation,
	title = "{REBEL}: Relation Extraction By End-to-end Language generation",
	author = "Huguet Cabot, Pere-Llu{\'\i}s  and
	Navigli, Roberto",
	booktitle = "Findings of the Association for Computational Linguistics: EMNLP 2021",
	month = nov,
	year = "2021",
	address = "Punta Cana, Dominican Republic",
	publisher = "Association for Computational Linguistics",
	url = "https://aclanthology.org/2021.findings-emnlp.204",
	pages = "2370--2381",
	abstract = "Extracting relation triplets from raw text is a crucial task in Information Extraction, enabling multiple applications such as populating or validating knowledge bases, factchecking, and other downstream tasks. However, it usually involves multiple-step pipelines that propagate errors or are limited to a small number of relation types. To overcome these issues, we propose the use of autoregressive seq2seq models. Such models have previously been shown to perform well not only in language generation, but also in NLU tasks such as Entity Linking, thanks to their framing as seq2seq tasks. In this paper, we show how Relation Extraction can be simplified by expressing triplets as a sequence of text and we present REBEL, a seq2seq model based on BART that performs end-to-end relation extraction for more than 200 different relation types. We show our model{'}s flexibility by fine-tuning it on an array of Relation Extraction and Relation Classification benchmarks, with it attaining state-of-the-art performance in most of them.",
}


@inproceedings{Siamese2015,
	title={Siamese Neural Networks for One-Shot Image Recognition},
	author={Gregory R. Koch},
	year={2015}
}

@inproceedings{tripletnetwork2015,
	title={Deep metric learning using triplet network},
	author={Hoffer, Elad and Ailon, Nir},
	booktitle={International workshop on similarity-based pattern recognition},
	pages={84--92},
	year={2015},
	organization={Springer}
}

@INPROCEEDINGS{7577578,
	author={Lahitani, Alfirna Rizqi and Permanasari, Adhistya Erna and Setiawan, Noor Akhmad},
	booktitle={2016 4th International Conference on Cyber and IT Service Management}, 
	title={Cosine similarity to determine similarity measure: Study case in online essay assessment}, 
	year={2016},
	volume={},
	number={},
	pages={1-6},
	doi={10.1109/CITSM.2016.7577578}}


@article{RandomForest,
	abstract = {Random forests are a combination of tree predictors such that each tree depends on the values of a random vector sampled independently and with the same distribution for all trees in the forest. The generalization error for forests converges a.s. to a limit as the number of trees in the forest becomes large. The generalization error of a forest of tree classifiers depends on the strength of the individual trees in the forest and the correlation between them. Using a random selection of features to split each node yields error rates that compare favorably to Adaboost (Y. Freund \& R. Schapire, Machine Learning: Proceedings of the Thirteenth International conference, ***, 148--156), but are more robust with respect to noise. Internal estimates monitor error, strength, and correlation and these are used to show the response to increasing the number of features used in the splitting. Internal estimates are also used to measure variable importance. These ideas are also applicable to regression.},
	author = {Breiman, Leo},
	date = {2001/10/01},
	date-added = {2022-08-09 22:24:28 +0200},
	date-modified = {2022-08-09 22:24:28 +0200},
	doi = {10.1023/A:1010933404324},
	id = {Breiman2001},
	isbn = {1573-0565},
	journal = {Machine Learning},
	number = {1},
	pages = {5--32},
	title = {Random Forests},
	url = {https://doi.org/10.1023/A:1010933404324},
	volume = {45},
	year = {2001},
	bdsk-url-1 = {https://doi.org/10.1023/A:1010933404324}}




@mastersthesis{Metzler-et-al2019,
	title = {Computer-assisted grading of short answers using word embeddings and keyphrase extraction.},
	author = {Tim Daniel Metzler, and Pl{\"o}ger, Paul G. and Gerhard Kraetzschmar.},
	
	school = {Hochschule Bonn-Rhein-Sieg},
	address = {Grantham-Allee 20, 53757 St. Augustin, Germany},
	year = {2019}
}


@dataset{NNExam,
	title={Neural Network Exam Dataset},
	author={Pl{\"o}ger, Paul G.},
	year={2020},
	school = {Hochschule Bonn-Rhein-Sieg},
	address = {Grantham-Allee 20, 53757 St. Augustin, Germany}
}

@dataset{AMRExam,
	title={Autonomous Mobile Robots Exam Dataset},
	author={Hochgeschwender, Nico},
	year={2021},
	school = {Hochschule Bonn-Rhein-Sieg},
	address = {Grantham-Allee 20, 53757 St. Augustin, Germany}
}

@mastersthesis{Zahid,
	title = {Explainable assistive short answer grading.},
	author = {Md Zahiduzzaman},
	school = {Bonn-Rhein-Sieg University of AppliedSciences},
	address = {Grantham-Allee 20, 53757 St. Augustin, Germany},
	year = {2020},
	month ={January}
}


@book{Benesty2009,
	abstract = {This chapter develops several forms of the Pearson correlation coefficient in the different domains. This coefficient can be used as an optimization criterion to derive different optimal noise reduction filters [14], but is even more useful for analyzing these optimal filters for their noise reduction performance.},
	address = {Berlin, Heidelberg},
	author = {Benesty, Jacob and Chen, Jingdong and Huang, Yiteng and Cohen, Israel},
	booktitle = {Noise Reduction in Speech Processing},
	doi = {10.1007/978-3-642-00296-0_5},
	isbn = {978-3-642-00296-0},
	pages = {1--4},
	publisher = {Springer Berlin Heidelberg},
	title = {Pearson Correlation Coefficient},
	url = {https://doi.org/10.1007/978-3-642-00296-0_5},
	year = {2009},
	bdsk-url-1 = {https://doi.org/10.1007/978-3-642-00296-0_5}
}

@article{Levinson1946TheW,
	title={The Wiener (Root Mean Square) Error Criterion in Filter Design and Prediction},
	author={Norman Levinson},
	journal={Journal of Mathematics and Physics},
	year={1946},
	volume={25},
	pages={261-278}
}


@article{kuhn1955hungarian,
	title={The Hungarian method for the assignment problem},
	author={Kuhn, Harold W},
	journal={Naval research logistics quarterly},
	volume={2},
	number={1-2},
	pages={83--97},
	year={1955},
	publisher={Wiley Online Library}
}


@book{bird2009natural,
	title={Natural language processing with Python: analyzing text with the natural language toolkit},
	author={Bird, Steven and Klein, Ewan and Loper, Edward},
	year={2009},
	publisher={" O'Reilly Media, Inc."}
}